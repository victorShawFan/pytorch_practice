{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用sklearn工具快速的实现多标签文本分类打下baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一个用KNN做iris的快速入门例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''准备工作'''\n",
    "# 引入数据集，sklearn包含众多数据集\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "# 将数据分为测试集和训练集\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 利用邻近点方式训练数据\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# 引入数据,本次导入鸢尾花数据，iris数据包含4个特征变量\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils.Bunch'>\n",
      "data\n",
      "target\n",
      "frame\n",
      "target_names\n",
      "DESCR\n",
      "feature_names\n",
      "filename\n"
     ]
    }
   ],
   "source": [
    "print(type(iris))\n",
    "for thing in iris:\n",
    "    print(thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\sklearn\\datasets\\data\\iris.csv\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "print(iris.filename)\n",
    "print(iris.feature_names)\n",
    "print(iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'查看函数说明的两大法宝: dir help'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dir(datasets)\n",
    "# help(datasets.load_iris)\n",
    "'''查看函数说明的两大法宝: dir help'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "特征变量的长度 150\n",
      "每一行的长度 4\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "[5.1 3.5 1.4 0.2]\n"
     ]
    }
   ],
   "source": [
    "# 特征变量\n",
    "iris_X = iris.data\n",
    "print(type(iris_X))\n",
    "print('特征变量的长度',len(iris_X))\n",
    "print('每一行的长度',len(iris_X[0]))\n",
    "print(iris.feature_names)\n",
    "print(iris_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "鸢尾花的目标值 150\n",
      "['setosa' 'versicolor' 'virginica']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "# 目标值\n",
    "iris_y = iris.target\n",
    "print(type(iris_y))\n",
    "print('鸢尾花的目标值',len(iris_y))\n",
    "print(iris.target_names)\n",
    "print(iris_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 1 1 0 1 1 2 1 2 0 0 1 2 0 0 1 0 1 2 0 2 1 1 0 2 1 0 1 2 1 0 1 2 1 1 2\n",
      " 2 1 0 0 2 2 0 0 1 0 1 1 0 1 1 1 1 0 2 1 0 1 1 1 0 2 2 1 2 0 2 0 2 0 0 2 0\n",
      " 2 2 1 1 0 2 0 2 2 2 1 1 2 1 2 1 1 2 0 1 0 1 2 0 1 2 1 0 0 2 2]\n"
     ]
    }
   ],
   "source": [
    "# 利用train_test_split进行训练集和测试机进行分开，test_size占30%\n",
    "X_train,X_test,y_train,y_test=train_test_split(iris_X,iris_y,test_size=0.3)\n",
    "# 我们看到训练数据的特征值分为3类\n",
    "print(y_train)\n",
    "# print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "# 一步就train好\n",
    "knn = KNeighborsClassifier()\n",
    "# 进行填充测试数据进行训练\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "params = knn.get_params()\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测得分为：0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "score = knn.score(X_test,y_test)\n",
    "print(\"预测得分为：%s\"%score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 2 0 1 2 2 0 2 2 0 2 2 2 1 1 0 0 2 0 1 0 1 0 1 0 0 0 0 1 0 2 2 0 1 1\n",
      " 2 0 1 2 2 1 0 0]\n",
      "[1 0 2 2 0 2 2 2 0 2 2 0 2 2 2 1 1 0 0 2 0 1 0 1 0 1 0 0 0 0 1 0 2 2 0 2 1\n",
      " 2 0 1 2 2 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# 预测数据，预测特征值\n",
    "print(knn.predict(X_test))\n",
    "# 打印真实特征值\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all, over. Faking easy man"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本例子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用二分类情感分析的数据集做，理论上来说，将对应的文本和标签转换成两个ndarray就可以，可以使用自己创建vocab和tokenize的方式，也可以使用torch_bert的tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import fanTools\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 第一步: Tokenize\n",
    "# sen = \"我今天中午去餐车吃了鸡肉饭和一瓶饮料\"\n",
    "# def sentence_get_BERT_Token(samples):\n",
    "#     '''利用online的库做tokenize，适合小数据'''\n",
    "#     import torch\n",
    "#     from pytorch_transformers import BertTokenizer\n",
    "#     model_name = 'bert-base-chinese'\n",
    "#     tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "#     tokenized_text = [tokenizer.tokenize(i) for i in samples]\n",
    "#     input_ids = [tokenizer.convert_tokens_to_ids(i) for i in tokenized_text]\n",
    "#     res = []\n",
    "#     for i in input_ids:\n",
    "#         res.extend(i)\n",
    "#     return res\n",
    "# res = sentence_get_BERT_Token(sen)\n",
    "# res\n",
    "# # g = np.array(res,dtype=int)\n",
    "# # g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本分类数据集路径\n",
    "posdata_path = '../re_datasets/clothing_comment_情感分析/posdata.txt'\n",
    "negdata_path = '../re_datasets/clothing_comment_情感分析/negdata.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive instance 质量好,做工也不错,尺码标准,\n",
      "negative instance 穿上不舒服，颜色和质感跟图片差异很大，建议慎重购买！后悔了！\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1500, 1500)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读入数据集\n",
    "pos_list = fanTools.read_file_to_list(posdata_path)\n",
    "neg_list = fanTools.read_file_to_list(negdata_path)\n",
    "print('positive instance',pos_list[0])\n",
    "print('negative instance',neg_list[0])\n",
    "len(pos_list), len(neg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~.,;《》？！“”‘’@#￥%…&×（）——+【】{};；●，。&～、|\\\\s:：'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "punc = punctuation + u'.,;《》？！“”‘’@#￥%…&×（）——+【】{};；●，。&～、|\\s:：'\n",
    "punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def rm_marks(str):\n",
    "    str1 = re.sub(r\"[{}]\".format(punc),'',str)\n",
    "    return str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['质量是我想要的，男票穿着正好！下次有机会还来', 1],\n",
       " ['客服态度很好，裤子质量也不错，物流也快。很值的一次购物！！好评！！', 1],\n",
       " ['大小合适，质量不错，穿在身上效果很好', 1],\n",
       " ['挺差的，会起毛球', 0],\n",
       " ['穿上感觉挺舒服，这裤子质量还不错', 1],\n",
       " ['我实在想不通这样的裤子怎么达到98的好评率，特粘毛，还显脏。难道京东要像*台一样假货横行？人家越做越好，你呢', 0],\n",
       " ['你们发货没看码数的？', 0],\n",
       " ['差评给物流，看到已签收，不打电话还不知道签到哪里了，很不满意', 0],\n",
       " ['衣服穿在身上挺舒适的，满意的一次购物', 1],\n",
       " ['质量差，有色差，联系客服全是专业术语回复，无语，差差差差评', 0]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "# data_all = []\n",
    "# label_all = []\n",
    "pos_all = [[i,1] for i in pos_list]\n",
    "neg_all = [[j,0] for j in neg_list]\n",
    "data_all = pos_all + neg_all\n",
    "random.shuffle(data_all)\n",
    "data_all[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['质量是我想要的，男票穿着正好！下次有机会还来',\n",
       "  '客服态度很好，裤子质量也不错，物流也快。很值的一次购物！！好评！！',\n",
       "  '大小合适，质量不错，穿在身上效果很好',\n",
       "  '挺差的，会起毛球',\n",
       "  '穿上感觉挺舒服，这裤子质量还不错'],\n",
       " [1, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_X = [i[0] for i in data_all]\n",
    "data_y = [i[1] for i in data_all]\n",
    "data_X[:5], data_y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:02, 1177.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''本地的bert来做tokenize'''\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "bert_path = \"../pretrained_model/chinese_roberta_wwm_ext_pytorch/\"  # 该文件夹下存放三个文件（'vocab.txt', 'pytorch_model.bin', 'config.json'）\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_path,\n",
    "                                          local_files_only=True)  # 初始化分词器\n",
    "data_X1, input_masks, input_types, = [], [], []\n",
    "labels = []  # 标签\n",
    "maxlen = 40  # 取30即可覆盖99%\n",
    "\n",
    "for i, line in tqdm(enumerate(data_X)):\n",
    "    title = line\n",
    "    encode_dict = tokenizer.encode_plus(text=title,\n",
    "                                        max_length=maxlen,\n",
    "                                        padding='max_length',\n",
    "                                        truncation=True)\n",
    "#     print(encode_dict['input_ids'])\n",
    "    data_X1.append(encode_dict['input_ids'])\n",
    "\n",
    "len(data_X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 6821, 784, 720, 6175, 2094, 8024, 743, 1726, 3341, 6407, 6407, 8024, 2218, 3766, 4959, 6814, 8024, 3332, 3160, 2345, 4638, 679, 6121, 8024, 6820, 3766, 6125, 6804, 1765, 3033, 8114, 2571, 671, 816, 4638, 1962, 511, 102, 0]\n"
     ]
    }
   ],
   "source": [
    "# 利用train_test_split进行训练集和测试机进行分开，test_size占30%\n",
    "X_train,X_test,y_train,y_test=train_test_split(data_X1,data_y,test_size=0.3)\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2100, 900, 2100, 900)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train),len(X_test),len(y_train),len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理 - sklearn的trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.50659134e-01, 4.69483568e-03],\n",
       "       [9.99686127e-01, 4.69483568e-03],\n",
       "       [4.76365348e+00, 4.69483568e-03],\n",
       "       [1.33678594e+00, 4.69483568e-03]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 为了使得训练数据的标准化规则与测试数据的标准化规则同步，preprocessing中提供了很多Scaler：\n",
    "data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n",
    "train_data = [[66, 213], [77, 0], [99, 1], [3252, 1]]\n",
    "test_data = [[546, 1], [3251, 1], [15243, 1], [4325, 1]]\n",
    "# 1. 基于mean和std的标准化\n",
    "scaler = preprocessing.StandardScaler().fit(train_data)\n",
    "scaler.transform(train_data)\n",
    "scaler.transform(test_data)\n",
    "\n",
    "# 2. 将每个特征值归一化到一个固定范围\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(0, 1)).fit(train_data)\n",
    "scaler.transform(train_data)\n",
    "scaler.transform(test_data)\n",
    "#feature_range: 定义归一化范围，注用（）括起来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正则化（normalize）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40824829, -0.40824829,  0.81649658],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.70710678, -0.70710678]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 当你想要计算两个样本的相似度时必不可少的一个操作，就是正则化。其思想是：首先求出样本的p-范数，然后该样本的所有元素都要除以该范数，这样最终使得每个样本的范数都为1。\n",
    "X = [[ 1., -1.,  2.],\n",
    "     [ 2.,  0.,  0.],\n",
    "     [ 0.,  1., -1.]]\n",
    "X_normalized = preprocessing.normalize(X, norm='l2')\n",
    "X_normalized                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 1., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 1., 0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 1., 0., 1., 0., 0.],\n",
       "       [0., 1., 1., 0., 0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot编码是一种对离散特征值的编码方式，在LR模型中常用到，用于给线性模型增加非线性能力\n",
    "data = [[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]]\n",
    "encoder = preprocessing.OneHotEncoder().fit(data)\n",
    "encoder.transform(data).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集拆分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 　在得到训练数据集时，通常我们经常会把训练数据集进一步拆分成训练集和验证集，这样有助于我们模型参数的选取。\n",
    "# 作用：将数据集划分为 训练集和测试集\n",
    "# 格式：train_test_split(*arrays, **options)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\"\"\"\n",
    "参数\n",
    "---\n",
    "arrays：样本数组，包含特征向量和标签\n",
    "\n",
    "test_size：\n",
    "　　float-获得多大比重的测试样本 （默认：0.25）\n",
    "　　int - 获得多少个测试样本\n",
    "\n",
    "train_size: 同test_size\n",
    "\n",
    "random_state:\n",
    "　　int - 随机种子（种子固定，实验可复现）\n",
    "　　\n",
    "shuffle - 是否在分割之前对数据进行洗牌（默认True）\n",
    "\n",
    "返回\n",
    "---\n",
    "分割后的列表，长度=2*len(arrays), \n",
    "　　(train-test split)\n",
    "\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型的选择与训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'alpha': 0.0001,\n",
       " 'batch_size': 'auto',\n",
       " 'beta_1': 0.9,\n",
       " 'beta_2': 0.999,\n",
       " 'early_stopping': True,\n",
       " 'epsilon': 1e-08,\n",
       " 'hidden_layer_sizes': (2000, 2000),\n",
       " 'learning_rate': 'constant',\n",
       " 'learning_rate_init': 0.001,\n",
       " 'max_fun': 15000,\n",
       " 'max_iter': 200,\n",
       " 'momentum': 0.9,\n",
       " 'n_iter_no_change': 10,\n",
       " 'nesterovs_momentum': True,\n",
       " 'power_t': 0.5,\n",
       " 'random_state': 1,\n",
       " 'shuffle': True,\n",
       " 'solver': 'adam',\n",
       " 'tol': 0.0001,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': 10,\n",
       " 'warm_start': True}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLP多层感知机（神经网络）的解法\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "model = MLPClassifier(solver='adam',hidden_layer_sizes=(2000,2000),verbose=10, \n",
    "                      random_state=1,warm_start=True, early_stopping=True)\n",
    "\"\"\"参数\n",
    "---\n",
    "    hidden_layer_sizes: 元祖\n",
    "    activation：激活函数\n",
    "    solver ：优化算法{‘lbfgs’, ‘sgd’, ‘adam’}\n",
    "    alpha：L2惩罚(正则化项)参数。\n",
    "\"\"\"\n",
    "params = model.get_params()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'copy_X': True, 'fit_intercept': True, 'n_jobs': None, 'normalize': True}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 线性回归\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# 定义线性回归模型\n",
    "model = LinearRegression(fit_intercept=True, normalize=False, \n",
    "    copy_X=True, n_jobs=1)\n",
    "\"\"\"\n",
    "参数\n",
    "---\n",
    "    fit_intercept：是否计算截距。False-模型没有截距\n",
    "    normalize： 当fit_intercept设置为False时，该参数将被忽略。 如果为真，则回归前的回归系数X将通过减去平均值并除以l2-范数而归一化。\n",
    "     n_jobs：指定线程数\n",
    "\"\"\"\n",
    "params = model.get_params()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  逻辑回归LR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# 定义逻辑回归模型\n",
    "\"\"\"参数\n",
    "---\n",
    "    penalty：使用指定正则化项（默认：l2）\n",
    "    dual: n_samples > n_features取False（默认）\n",
    "    C：正则化强度的反，值越小正则化强度越大\n",
    "    n_jobs: 指定线程数\n",
    "    random_state：随机数生成器\n",
    "    fit_intercept: 是否需要常量\n",
    "\"\"\"\n",
    "model = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, \n",
    "    fit_intercept=True, intercept_scaling=1, class_weight=None, \n",
    "    random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', \n",
    "    verbose=0, warm_start=False, n_jobs=1)\n",
    "params = model.get_params()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'break_ties': False,\n",
       " 'cache_size': 200,\n",
       " 'class_weight': None,\n",
       " 'coef0': 0.0,\n",
       " 'decision_function_shape': 'ovr',\n",
       " 'degree': 3,\n",
       " 'gamma': 'auto',\n",
       " 'kernel': 'rbf',\n",
       " 'max_iter': -1,\n",
       " 'probability': False,\n",
       " 'random_state': None,\n",
       " 'shrinking': True,\n",
       " 'tol': 0.001,\n",
       " 'verbose': False}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 支持向量机SVM\n",
    "from sklearn.svm import SVC\n",
    "model = SVC(C=1.0, kernel='rbf', gamma='auto')\n",
    "\"\"\"参数\n",
    "---\n",
    "    C：误差项的惩罚参数C\n",
    "    gamma: 核相关系数。浮点数，If gamma is ‘auto’ then 1/n_features will be used instead.\n",
    "\"\"\"\n",
    "params = model.get_params()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 1\n",
    "bool(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  朴素贝叶斯算法NB\n",
    "from sklearn import naive_bayes\n",
    "\"\"\"\n",
    "文本分类问题常用MultinomialNB\n",
    "参数\n",
    "---\n",
    "    alpha：平滑参数\n",
    "    fit_prior：是否要学习类的先验概率；false-使用统一的先验概率\n",
    "    class_prior: 是否指定类的先验概率；若指定则不能根据参数调整\n",
    "    binarize: 二值化的阈值，若为None，则假设输入由二进制向量组成\n",
    "\"\"\"\n",
    "model = naive_bayes.GaussianNB() # 高斯贝叶斯\n",
    "# model = naive_bayes.MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
    "# model = naive_bayes.BernoulliNB(alpha=1.0, binarize=0.0, fit_prior=True, class_prior=None)\n",
    "params = model.get_params()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 决策树DT\n",
    "from  sklearn  import  tree \n",
    "\"\"\"参数\n",
    "---\n",
    "    criterion ：特征选择准则gini/entropy\n",
    "    max_depth：树的最大深度，None-尽量下分\n",
    "    min_samples_split：分裂内部节点，所需要的最小样本树\n",
    "    min_samples_leaf：叶子节点所需要的最小样本数\n",
    "    max_features: 寻找最优分割点时的最大特征数\n",
    "    max_leaf_nodes：优先增长到最大叶子节点数\n",
    "    min_impurity_decrease：如果这种分离导致杂质的减少大于或等于这个值，则节点将被拆分。\n",
    "\"\"\"\n",
    "model = tree.DecisionTreeClassifier(criterion=’gini’, max_depth=None, \n",
    "    min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "    max_features=None, random_state=None, max_leaf_nodes=None, \n",
    "    min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "     class_weight=None, presort=False)\n",
    "params = model.get_params()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn\n",
    "from sklearn import neighbors\n",
    "#定义kNN分类模型\n",
    "model = neighbors.KNeighborsClassifier(n_neighbors=5, n_jobs=1) # 分类\n",
    "# model = neighbors.KNeighborsRegressor(n_neighbors=5, n_jobs=1) # 回归\n",
    "\"\"\"参数\n",
    "---\n",
    "    n_neighbors： 使用邻居的数目\n",
    "    n_jobs：并行任务数\n",
    "\"\"\"\n",
    "params = model.get_params()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "model = PCA(n_components=0.95)\n",
    "params = model.get_params()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means\n",
    "from sklearn.cluster import KMeans\n",
    "model = KMeans(n_clusters=10, random_state=0)\n",
    "params = model.get_params()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class KMeans in module sklearn.cluster._kmeans:\n",
      "\n",
      "class KMeans(sklearn.base.TransformerMixin, sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      " |  KMeans(n_clusters=8, *, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='deprecated', verbose=0, random_state=None, copy_x=True, n_jobs='deprecated', algorithm='auto')\n",
      " |  \n",
      " |  K-Means clustering.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <k_means>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  \n",
      " |  n_clusters : int, default=8\n",
      " |      The number of clusters to form as well as the number of\n",
      " |      centroids to generate.\n",
      " |  \n",
      " |  init : {'k-means++', 'random', ndarray, callable}, default='k-means++'\n",
      " |      Method for initialization:\n",
      " |  \n",
      " |      'k-means++' : selects initial cluster centers for k-mean\n",
      " |      clustering in a smart way to speed up convergence. See section\n",
      " |      Notes in k_init for more details.\n",
      " |  \n",
      " |      'random': choose `n_clusters` observations (rows) at random from data\n",
      " |      for the initial centroids.\n",
      " |  \n",
      " |      If an ndarray is passed, it should be of shape (n_clusters, n_features)\n",
      " |      and gives the initial centers.\n",
      " |  \n",
      " |      If a callable is passed, it should take arguments X, n_clusters and a\n",
      " |      random state and return an initialization.\n",
      " |  \n",
      " |  n_init : int, default=10\n",
      " |      Number of time the k-means algorithm will be run with different\n",
      " |      centroid seeds. The final results will be the best output of\n",
      " |      n_init consecutive runs in terms of inertia.\n",
      " |  \n",
      " |  max_iter : int, default=300\n",
      " |      Maximum number of iterations of the k-means algorithm for a\n",
      " |      single run.\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      Relative tolerance with regards to Frobenius norm of the difference\n",
      " |      in the cluster centers of two consecutive iterations to declare\n",
      " |      convergence.\n",
      " |  \n",
      " |  precompute_distances : {'auto', True, False}, default='auto'\n",
      " |      Precompute distances (faster but takes more memory).\n",
      " |  \n",
      " |      'auto' : do not precompute distances if n_samples * n_clusters > 12\n",
      " |      million. This corresponds to about 100MB overhead per job using\n",
      " |      double precision.\n",
      " |  \n",
      " |      True : always precompute distances.\n",
      " |  \n",
      " |      False : never precompute distances.\n",
      " |  \n",
      " |      .. deprecated:: 0.23\n",
      " |          'precompute_distances' was deprecated in version 0.22 and will be\n",
      " |          removed in 0.25. It has no effect.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Verbosity mode.\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      Determines random number generation for centroid initialization. Use\n",
      " |      an int to make the randomness deterministic.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  copy_x : bool, default=True\n",
      " |      When pre-computing distances it is more numerically accurate to center\n",
      " |      the data first. If copy_x is True (default), then the original data is\n",
      " |      not modified. If False, the original data is modified, and put back\n",
      " |      before the function returns, but small numerical differences may be\n",
      " |      introduced by subtracting and then adding the data mean. Note that if\n",
      " |      the original data is not C-contiguous, a copy will be made even if\n",
      " |      copy_x is False. If the original data is sparse, but not in CSR format,\n",
      " |      a copy will be made even if copy_x is False.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of OpenMP threads to use for the computation. Parallelism is\n",
      " |      sample-wise on the main cython loop which assigns each sample to its\n",
      " |      closest center.\n",
      " |  \n",
      " |      ``None`` or ``-1`` means using all processors.\n",
      " |  \n",
      " |      .. deprecated:: 0.23\n",
      " |          ``n_jobs`` was deprecated in version 0.23 and will be removed in\n",
      " |          0.25.\n",
      " |  \n",
      " |  algorithm : {\"auto\", \"full\", \"elkan\"}, default=\"auto\"\n",
      " |      K-means algorithm to use. The classical EM-style algorithm is \"full\".\n",
      " |      The \"elkan\" variation is more efficient on data with well-defined\n",
      " |      clusters, by using the triangle inequality. However it's more memory\n",
      " |      intensive due to the allocation of an extra array of shape\n",
      " |      (n_samples, n_clusters).\n",
      " |  \n",
      " |      For now \"auto\" (kept for backward compatibiliy) chooses \"elkan\" but it\n",
      " |      might change in the future for a better heuristic.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |          Added Elkan algorithm\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  cluster_centers_ : ndarray of shape (n_clusters, n_features)\n",
      " |      Coordinates of cluster centers. If the algorithm stops before fully\n",
      " |      converging (see ``tol`` and ``max_iter``), these will not be\n",
      " |      consistent with ``labels_``.\n",
      " |  \n",
      " |  labels_ : ndarray of shape (n_samples,)\n",
      " |      Labels of each point\n",
      " |  \n",
      " |  inertia_ : float\n",
      " |      Sum of squared distances of samples to their closest cluster center.\n",
      " |  \n",
      " |  n_iter_ : int\n",
      " |      Number of iterations run.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  \n",
      " |  MiniBatchKMeans\n",
      " |      Alternative online implementation that does incremental updates\n",
      " |      of the centers positions using mini-batches.\n",
      " |      For large scale learning (say n_samples > 10k) MiniBatchKMeans is\n",
      " |      probably much faster than the default batch implementation.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The k-means problem is solved using either Lloyd's or Elkan's algorithm.\n",
      " |  \n",
      " |  The average complexity is given by O(k n T), were n is the number of\n",
      " |  samples and T is the number of iteration.\n",
      " |  \n",
      " |  The worst case complexity is given by O(n^(k+2/p)) with\n",
      " |  n = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii,\n",
      " |  'How slow is the k-means method?' SoCG2006)\n",
      " |  \n",
      " |  In practice, the k-means algorithm is very fast (one of the fastest\n",
      " |  clustering algorithms available), but it falls in local minima. That's why\n",
      " |  it can be useful to restart it several times.\n",
      " |  \n",
      " |  If the algorithm stops before fully converging (because of ``tol`` or\n",
      " |  ``max_iter``), ``labels_`` and ``cluster_centers_`` will not be consistent,\n",
      " |  i.e. the ``cluster_centers_`` will not be the means of the points in each\n",
      " |  cluster. Also, the estimator will reassign ``labels_`` after the last\n",
      " |  iteration to make ``labels_`` consistent with ``predict`` on the training\n",
      " |  set.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  \n",
      " |  >>> from sklearn.cluster import KMeans\n",
      " |  >>> import numpy as np\n",
      " |  >>> X = np.array([[1, 2], [1, 4], [1, 0],\n",
      " |  ...               [10, 2], [10, 4], [10, 0]])\n",
      " |  >>> kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
      " |  >>> kmeans.labels_\n",
      " |  array([1, 1, 1, 0, 0, 0], dtype=int32)\n",
      " |  >>> kmeans.predict([[0, 0], [12, 3]])\n",
      " |  array([1, 0], dtype=int32)\n",
      " |  >>> kmeans.cluster_centers_\n",
      " |  array([[10.,  2.],\n",
      " |         [ 1.,  2.]])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      KMeans\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.ClusterMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_clusters=8, *, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='deprecated', verbose=0, random_state=None, copy_x=True, n_jobs='deprecated', algorithm='auto')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None, sample_weight=None)\n",
      " |      Compute k-means clustering.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training instances to cluster. It must be noted that the data\n",
      " |          will be converted to C ordering, which will cause a memory\n",
      " |          copy if the given data is not C-contiguous.\n",
      " |          If a sparse matrix is passed, a copy will be made if it's not in\n",
      " |          CSR format.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |          Not used, present here for API consistency by convention.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          The weights for each observation in X. If None, all observations\n",
      " |          are assigned equal weight.\n",
      " |      \n",
      " |          .. versionadded:: 0.20\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  fit_predict(self, X, y=None, sample_weight=None)\n",
      " |      Compute cluster centers and predict cluster index for each sample.\n",
      " |      \n",
      " |      Convenience method; equivalent to calling fit(X) followed by\n",
      " |      predict(X).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          New data to transform.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |          Not used, present here for API consistency by convention.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          The weights for each observation in X. If None, all observations\n",
      " |          are assigned equal weight.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      labels : ndarray of shape (n_samples,)\n",
      " |          Index of the cluster each sample belongs to.\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, sample_weight=None)\n",
      " |      Compute clustering and transform X to cluster-distance space.\n",
      " |      \n",
      " |      Equivalent to fit(X).transform(X), but more efficiently implemented.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          New data to transform.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |          Not used, present here for API consistency by convention.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          The weights for each observation in X. If None, all observations\n",
      " |          are assigned equal weight.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : array of shape (n_samples, n_clusters)\n",
      " |          X transformed in the new space.\n",
      " |  \n",
      " |  predict(self, X, sample_weight=None)\n",
      " |      Predict the closest cluster each sample in X belongs to.\n",
      " |      \n",
      " |      In the vector quantization literature, `cluster_centers_` is called\n",
      " |      the code book and each value returned by `predict` is the index of\n",
      " |      the closest code in the code book.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          New data to predict.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          The weights for each observation in X. If None, all observations\n",
      " |          are assigned equal weight.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      labels : ndarray of shape (n_samples,)\n",
      " |          Index of the cluster each sample belongs to.\n",
      " |  \n",
      " |  score(self, X, y=None, sample_weight=None)\n",
      " |      Opposite of the value of X on the K-means objective.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          New data.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |          Not used, present here for API consistency by convention.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          The weights for each observation in X. If None, all observations\n",
      " |          are assigned equal weight.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Opposite of the value of X on the K-means objective.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Transform X to a cluster-distance space.\n",
      " |      \n",
      " |      In the new space, each dimension is the distance to the cluster\n",
      " |      centers.  Note that even if X is sparse, the array returned by\n",
      " |      `transform` will typically be dense.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          New data to transform.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray of shape (n_samples, n_clusters)\n",
      " |          X transformed in the new space.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(KMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测得分为：0.7011111111111111\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train,y_train)\n",
    "score = model.score(X_test,y_test)\n",
    "print(\"预测得分为：%s\"%score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "sen = \"质量不错，尺码大小合适，很喜欢\"\n",
    "aa = tokenizer.encode_plus(text=sen, max_length=maxlen, padding='max_length', truncation=True)\n",
    "token_sen = [aa[\"input_ids\"]]\n",
    "print(model.predict(token_sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "sen = \"太好了，很棒啊，太厉害了\"\n",
    "aa = tokenizer.encode_plus(text=sen, max_length=maxlen, padding='max_length', truncation=True)\n",
    "token_sen = [aa[\"input_ids\"]]\n",
    "print(model.predict(token_sen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型评估与选择篇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.71111111, 0.65555556, 0.65      , 0.78333333, 0.69444444])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\"\"\"参数\n",
    "---\n",
    "    model：拟合数据的模型\n",
    "    cv ： k-fold\n",
    "    scoring: 打分参数-‘accuracy’、‘f1’、‘precision’、‘recall’ 、‘roc_auc’、'neg_log_loss'等等\n",
    "\"\"\"\n",
    "cross_val_score(model, X_test, y_test, scoring=None, cv=None, n_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 检验曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用检验曲线，我们可以更加方便的改变模型参数，获取模型表现。\n",
    "from sklearn.model_selection import validation_curve\n",
    "\"\"\"参数\n",
    "---\n",
    "    model:用于fit和predict的对象\n",
    "    X, y: 训练集的特征和标签\n",
    "    param_name：将被改变的参数的名字\n",
    "    param_range： 参数的改变范围\n",
    "    cv：k-fold\n",
    "   \n",
    "返回值\n",
    "---\n",
    "   train_score: 训练集得分（array）\n",
    "    test_score: 验证集得分（array）\n",
    "\"\"\"\n",
    "train_score, test_score = validation_curve(model, X_test, y_test, params, param_range=(1,100), cv=None, scoring=None, n_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们可以将我们训练好的model保存到本地，或者放到线上供用户使用，那么如何保存训练好的model呢？主要有下面两种方式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存为pickle文件\n",
    "import pickle\n",
    "\n",
    "# 保存模型\n",
    "with open('model.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "# 读取模型\n",
    "with open('model.pickle', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn自带方法joblib\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# 保存模型\n",
    "joblib.dump(model, 'model.pickle')\n",
    "\n",
    "#载入模型\n",
    "model = joblib.load('model.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Deep]",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
